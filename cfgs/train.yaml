use_ddp: False   # whether to use DataDistributedParallel, for multi-gpus.
port:   # the port for the DataDistributedParallel training.

resume:
work_dir: mxfont/result   # the directory to save checkpoints, validation images, and the log.

decomposition: mxfont/data/kz_decomposition.json  # path to the "decomposition rule" file.
primals: mxfont/data/kz_primals.json   # path to the "primals" file.

pretrained: False
pretrained_path: mxfont/generator1.pth

dset:   # leave blank
  train:   # leave blank
    data_dir: mxfont/data/ttfs/train   # path to .ttf files for the training
  val:   # leave blank
    data_dir: mxfont/data/ttfs/val  # path to .ttf files for the validation
    source_font: mxfont/data/ttfs/val/arial.ttf   # path to the .ttf file used as the source font during the validation

#TODOLIST
early_stopping:
  patience: 5  # Number of epochs with no improvement after which training will be stopped
  min_delta: 0.001  # Minimum change in the monitored quantity to qualify as an improvement
